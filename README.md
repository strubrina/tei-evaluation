# TEI LLM Evaluation

Multi-dimensional evaluation framework for assessing LLM-generated TEI XML quality.

## Overview

This project is a comprehensive framework for assessing the quality of TEI XML files generated by LLMs. It provides a multi-dimensional evaluation approach, covering syntactic correctness, source fidelity, schema compliance, structural fidelity, and semantic content matching. The framework is designed to:

- Validate XML well-formedness (Dimension 0)
- Verify content preservation against source text (Dimension 1)
- Check compliance with TEI and/or project-specific schemas using RelaxNG validation (Dimension 2)
- Compare the structural organization and completeness of LLM outputs against reference XML files (Dimension 3)
- Evaluate element-level semantic content matching between LLM outputs and references (Dimension 4)

The evaluation process is modular, allowing independent or unified assessment across all dimensions. Results are reported in both machine-readable (JSON) and human-friendly (Excel) formats, with support for detailed error analysis, scoring, and visualizations. The framework is suitable for benchmarking LLMs, refining prompt strategies, and ensuring high-quality TEI XML generation for digital humanities and related projects.

## Evaluation Dimensions

### Dimension 0: XML Well-formedness

Validates the syntactic correctness of XML structure without evaluating content.

**Evaluation Components**

- Validates XML structure using lxml parser
- Detects tag nesting, closure, and syntax errors
- Identifies character encoding and escaping issues
- Checks attribute syntax and document structure
- Categorizes errors into specific types (tag_structure, character_encoding, attributes)

**Pass/Fail Criteria**

Primary evaluation uses binary checks:

- **PASS**: Valid XML structure
- **FAIL**: Malformed XML

Additional metrics provided for analysis:
- XML Score: 100 - (error count) - informative score only, not included into final scoring
- Error breakdown by category


**Output Files**

- `json/d0_wellformedness_report.json` - Detailed JSON report
- `d0_wellformedness_report.xlsx` - Excel summary with error breakdown

### Dimension 1: Source Fidelity

Evaluates content preservation by comparing extracted text from the LLM-generated XML output against original source text.

**Evaluation Components**

- Compares original text with XML-extracted content
- Measures text similarity with/without whitespace normalization
- Detects missing, extra, or modified content
- Provides character-level difference analysis with context

**Pass/Fail Criteria**

- **PASS**: Exact match (whitespace ignored)
- **FAIL**: Content differs from source

**Scoring**

- Similarity percentage for quantifying content differences
- Character-level diff analysis

**Output Files**

- `json/d1_source_fidelity_report.json` - Detailed JSON report
- `d1_source_fidelity_report.xlsx` - Excel report with diff details

### Dimension 2: Schema Compliance

Validates XML/TEI files against formal schema definitions using Jing RelaxNG validator.

**Validation Types**

- **TEI Schema Validation**: Checks compliance with TEI P5 standard (bundled schema: `tei_all.rng`)
- **Project Schema Validation**: Validates against project-specific requirements (user schema in `data/schemas/`)

**Namespace Handling**

The evaluator includes namespace handling for LLM-generated output:

- **Automatic TEI Wrapping**: Files with `<text>` or `<body>` root elements are automatically wrapped with proper TEI structure
- **Namespace Declaration**: Adds required TEI namespace (`xmlns="http://www.tei-c.org/ns/1.0"`)
- **Minimal TEI Header**: Generates compliant `<teiHeader>` with fileDesc for validation purposes
- **Namespace-Agnostic Checking**: Element detection works with or without namespace prefixes

**Evaluation Modes**

- `evaluate_tei_only()` - TEI P5 schema validation only
- `evaluate_project_only()` - Project schema validation only
- `evaluate_full()` - Combined TEI + Project schema validation (requires both)

**Pass/Fail Criteria**

Primary evaluation uses binary checks:

- **TEI Validation**: PASS if conforms to TEI P5 / FAIL if schema violations detected
- **Project Validation**: PASS if meets project requirements / FAIL if violations detected

Additional metrics provided for analysis:
- TEI Score and Project Score (100 - error count) - Note: Error-based scoring has limitations as Jing reports errors sequentially and early structural violations may prevent detection of subsequent issues
- Jing error categorization (element_not_allowed, missing_required_element, invalid_attribute, content_model_violation)

**Reference Comparison** (Optional)

If reference files are available in `data/references/`:
- Validates reference files against same schemas
- Additionally compares validity status (LLM output vs. reference)
- Reports validity matches/mismatches

**Output Files**

Mode-specific reports are generated based on validation mode:
- **TEI-only mode**: `json/d2_tei_validation_report.json` + `d2_tei_validation_report.xlsx`
- **Project-only mode**: `json/d2_project_validation_report.json` + `d2_project_validation_report.xlsx`
- **Combined mode**: `json/d2_schema_validation_report.json` + `d2_schema_validation_report.xlsx` (unified report with both validations)

### Dimension 3: Structural Comparison

Compares structural organization and element composition of LLM-generated XML against reference XML files.

**Evaluation Components**

This dimension evaluates **two distinct aspects** of the complete file:

1. **Completeness**: Presence of all required element types with correct frequency
   - Compares total element counts (LLM output vs. reference)
   - Identifies added element types (present in LLM output but not in reference)
   - Identifies removed element types (missing from LLM output but present in reference)
   - Tracks per-type count differences (e.g., 5 `<p>` elements instead of 3)

2. **Structural Organization**: Proper organization and positioning of elements
   - Detects both structural and textual operations via XMLDiff: insertions, deletions, moves, renames, text changes
   - Evaluates only structural operations for scoring and pass/fail (InsertNode, DeleteNode, MoveNode, RenameNode, attribute changes)
   - Excludes text content changes from evaluation (UpdateTextIn, UpdateTextAfter) - text evaluated in Dimensions 1 and 4
   - Both operation types are detected and reported in output for reference

**Pass/Fail Criteria**

Document-level binary check based on tree structure similarity:

- **PASS (Structural Match)**: 100% LCS similarity (perfect tree structure match) AND no errors
- **FAIL (Structural Diff)**: LCS similarity < 100% OR errors present


**Scoring**

The primary score used in summaries and aggregations is the **Structural Score**:

- **Structural Score** (0-100%): Tree structure similarity (PRIMARY SCORE)
  - Based on Longest Common Subsequence (LCS) of element tree traversal
  - Measures whether elements appear in the correct hierarchical order
  - Formula: `LCS_Similarity × 100` where LCS_Similarity = LCS_length / max(xml_elements, ref_elements)
  - Falls back to Tree Edit Distance (TED) similarity if LCS unavailable
  - **This is the score used in summary statistics** (`avg_structural_score`)

**Additional Metrics** (calculated and reported but not used for primary scoring):

- **Completeness Score** (0-100%): Element type identification accuracy
  - Based on precision/recall of element types (micro-F1 score)
  - Measures whether all expected element types are present with correct frequencies
  - Formula: F1 score based on element type counts (TP, FP, FN per element type)

- **XMLDiff Score** (0-100%): Detailed structural difference analysis
  - Formula: `(1 - Structural_XMLDiff_Operations / Reference_Element_Count) × 100`
  - Counts: InsertNode, DeleteNode, MoveNode, RenameNode, attribute changes
  - Excludes: UpdateTextIn, UpdateTextAfter (text content changes)
  - Provides granular operation-level analysis for debugging


**Analysis Methods**

1. **Element Count Comparison**: Fast quantitative check of total element counts
2. **XMLDiff Analysis**: Detects all structural differences with operation-level detail
3. **Element Type Analysis**: Identifies which specific element types were added/removed/changed


**Reference File Requirement**

- Requires reference files in `data/references/` with matching filenames
- Files without references are skipped (not marked as failed)
- Reference availability shown in metrics

**Dependencies**

- **Required**: `lxml` for XML parsing
- **Optional**: `xmldiff` for detailed structural difference detection (recommended for complete analysis)

**Output Files**

- `json/d3_structural_fidelity_report.json` - Structural comparison results with element analysis
- `d3_structural_fidelity_report.xlsx` - Multi-sheet Excel with:
  - Summary statistics
  - File-by-file detailed results
  - Completeness analysis (element type breakdown)
  - XMLDiff operations (structural change details)

### Dimension 4: Semantic Content Matching

Evaluates whether the text content within structural elements matches the expected reference content at the element level.

**Evaluation Components**

This dimension performs **element-level content matching** with three operational modes:

1. **Auto-Discovery Mode** (Default - Genre-Agnostic)
   - Automatically discovers all text-bearing elements in both LLM output and reference
   - Identifies any element containing direct text content
   - Excludes wrapper-only elements (e.g., `<div>` containing only `<p>` without direct text)
   - Works across any TEI genre without predefined element lists

2. **Correspondence Mode** (Predefined Elements)
   - Evaluates predefined letter-specific elements: `opener`, `closer`, `salute`, `signed`, `dateline`, `address`, `p`, `persName`, `placeName`, `date`, `orgName`

3. **Custom Mode** (User-Specified Elements)
   - Accepts custom list of element types for evaluation
   - Allows targeted evaluation for specific project needs

**Content Matching Strategy**

The evaluator uses **Content-Based Best Matching**:

- Finds best content match regardless of element position
- Each LLM element is matched to the most similar reference element
- Detects reordered elements (matched but at different positions)
- Used for scoring and evaluation

**Match Types**:

1. **Exact Match**: Content is identical after normalization (whitespace ignored)
2. **Over-inclusion**: LLM content includes complete reference + extra text (reference found as substring in LLM output)
3. **Under-inclusion**: LLM content is substring of reference (partial capture)
4. **No Match**: No substring relationship found

**Practical Match Approach**

- **No hard thresholds applied** in D4 individual evaluation
- All substring relationships (over/under-inclusion) count as practical matches
- Ratios calculated for quantitative analysis but **NOT currently used in scoring**:
  - `inclusion_ratio`: What % of LLM content is the reference (for over-inclusion)
  - `coverage_ratio`: What % of reference was captured by LLM (for under-inclusion)
- Quality interpretation happens in **unified reporting** using D1 context:
  - If D1 content preserved → practical matches weighted 0.8 (likely boundary issues)
  - If D1 NOT preserved → practical matches weighted 0.6 (possible hallucinations)

**Pass/Fail Criteria**

File-level binary check:

- **PASS (Perfect Match)**: No errors detected (no missing elements, no extra elements)
  - All reference elements have corresponding matches in LLM output
  - No additional elements in LLM output beyond reference
  - Content quality (exact vs. practical match) does not affect pass/fail, only scoring
- **FAIL**: Any errors present (missing elements OR extra elements)

**Note**: A file can PASS even with practical matches (over/under-inclusion) as long as all elements are present and no extras exist. The scoring reflects content quality via Macro F1.

Additional metrics provided for analysis:
- Match type distribution (exact/over/under/none)
- Inclusion and coverage ratios for quality assessment
- Element-by-element match analysis
- Precision, Recall, and F1 scores (micro and macro)

**Scoring System**

Scoring is based on **Macro F1** from content-based best matching:

**Match Weighting**:
- **Exact matches**: Weighted at 1.0 (full credit)
- **Practical matches** (over/under-inclusion): Context-aware weighting
  - In standalone D4 evaluation: Fixed weight of 0.6 (conservative, assumes possible hallucinations)
  - In unified evaluation with D1 context:
    - If D1 content preserved → weight 0.8 (likely boundary issues only)
    - If D1 NOT preserved → weight 0.6 (possible hallucinations)
- **No matches**: 0.0 (no credit)

**Score Calculation**:
1. For each element type, calculate precision, recall, and F1 using weighted True Positives:
   - TP (True Positive) = exact_matches × 1.0 + practical_matches × weight
   - FP (False Positive) = extra elements in LLM output
   - FN (False Negative) = missing elements from reference
2. Calculate per-type F1 scores: `F1 = 2 × (Precision × Recall) / (Precision + Recall)`
3. **Overall Score = Macro F1**: Average of all per-type F1 scores (0-100)
   - Treats each element type equally regardless of frequency
   - Ensures rare elements are weighted equally with common elements

**Additional Metrics** (reported but not used for primary scoring):
- Micro F1: Weighted by element frequency
- Precision and Recall: Individual metrics for analysis
- Match type distribution: Breakdown of exact/practical/no matches

**Note**: Element count mismatches are NOT penalized in D4 - they are evaluated in Dimension 3 (Structural Comparison). D4 focuses exclusively on content quality of matched elements.

**Reference File Requirement**

- **Required**: Reference files in `data/references/` with matching filenames
- Files without references are skipped (not marked as failed)

**Dependencies**

- **lxml**: XML parsing and element extraction
- **TEI Wrapper**: Automatic TEI wrapping with namespace handling (same as D2)

**Output Files**

- `json/d4_semantic_validation_report.json` - Complete content matching results with element-level analysis
- `d4_semantic_validation_report.xlsx` - Multi-sheet Excel with:
  - Summary statistics (match type distribution, average scores)
  - File-by-file detailed results
  - Element-level match analysis (content comparisons, ratios)
  - Match type breakdown (exact/over/under/none counts)

## Installation

### Prerequisites

- **Python**: 3.9 or higher
- **Java**: Required for Dimension 2 (Schema Validation)
  - Jing RelaxNG validator is bundled (jing-RELEASE220.jar)
  - Java must be installed and available in system PATH
  - Verify with: `java -version`

### Setup Steps

1. **Create a virtual environment** (recommended)
   ```bash
   python -m venv .venv
   ```

2. **Activate the virtual environment**
   - Windows (PowerShell):
     ```powershell
     .\.venv\Scripts\Activate.ps1
     ```
   - Windows (CMD):
     ```cmd
     .venv\Scripts\activate.bat
     ```
   - Linux/Mac:
     ```bash
     source .venv/bin/activate
     ```

3. **Install Python dependencies**
   ```bash
   pip install -r requirements.txt
   ```

### Required Python Packages

The framework requires the following packages (specified in `requirements.txt`):

**Core Dependencies:**
- `lxml` (6.0.0) - XML parsing and manipulation (used in all dimensions)
- `openpyxl` (3.1.5) - Excel report generation
- `xmldiff` (2.5) - Structural comparison (Dimension 3)

**Data Analysis & Visualization:**
- `pandas` (2.3.1) - Data processing for reports
- `numpy` (2.0.2) - Numerical computations
- `matplotlib` (3.9.4) - Visualization generation
- `seaborn` (0.13.2) - Statistical visualizations

**Supporting Libraries:**
- `python-dateutil`, `pytz`, `tzdata` - Date/time handling
- Standard dependencies for visualization (contourpy, cycler, fonttools, kiwisolver, pillow, pyparsing, packaging)

### Further Components

- **Jing Validator**: Bundled at `tei_evaluator/resources/tools/jing-RELEASE220.jar`
  - No separate installation required
  - Requires Java Runtime Environment (JRE)


## Project Structure

```
tei_llm_evaluation/
├── tei_evaluator/              # Main evaluation package
│   ├── core/                   # Core evaluation modules
│   │   ├── base_evaluator.py  # Base evaluator class
│   │   ├── d0_syntactic.py    # D0: XML wellformedness
│   │   ├── d1_source.py       # D1: Source fidelity
│   │   ├── d2_schema.py       # D2: Schema compliance
│   │   ├── d3_structural.py   # D3: Structural comparison
│   │   └── d4_semantic.py     # D4: Semantic content matching
│   │
│   ├── models/                 # Data models
│   │   ├── evaluation_result.py   # EvaluationResult class
│   │   ├── error_types.py         # Error type definitions
│   │   └── metrics.py             # Metrics structures
│   │
│   ├── reporting/              # Report generation
│   │   ├── d0_report.py       # D0 reports (JSON + Excel)
│   │   ├── d1_report.py       # D1 reports (JSON + Excel)
│   │   ├── d2_report.py       # D2 reports (JSON + Excel)
│   │   ├── d3_report.py       # D3 reports (JSON + Excel)
│   │   ├── d4_report.py       # D4 reports (JSON + Excel)
│   │   ├── unified_report.py  # Unified cross-dimensional reports
│   │   └── visualization.py   # Visualization generators
│   │
│   ├── utils/                  # Utility modules
│   │   ├── logging_config.py       # Logging configuration
│   │   ├── jing_validator.py       # Jing RelaxNG validator wrapper
│   │   ├── tei_wrapper.py          # Automatic TEI wrapping
│   │   ├── content_preservation.py # Content comparison utilities
│   │   ├── structural_comparator.py # XMLDiff integration
│   │   ├── lxml_error_handler.py   # XML error parsing
│   │   └── ...                     # Additional analyzers
│   │
│   └── resources/              # Bundled resources
│       ├── schemas/            # TEI and project schemas
│       │   └── tei_all.rng    # TEI P5 RelaxNG schema
│       └── tools/
│           └── jing-RELEASE220.jar  # Jing validator
│
├── data/                       # Evaluation data (user-provided)
│   ├── input/                  # Source text files (.txt)
│   ├── output/                 # LLM-generated XML organized by model
│   │   ├── model-name-1/
│   │   │   └── processing_YYYYMMDD_HHMMSS/  # Timestamped processing runs
│   │   │       ├── *.xml      # Generated XML files
│   │   │       ├── processing_metadata.json  # Run metadata
│   │   │       └── prompts.txt  # Prompts used (if applicable)
│   │   └── model-name-2/
│   ├── references/             # Reference XML files (gold standard)
│   ├── schemas/                # Project-specific schemas
│   │   └── letter_schema.rng  # Custom schema (if any)
│   └── examples/               # Example data for testing/demonstration
│
├── results/                    # Evaluation results (generated)
│   ├── model/                  # Per-model evaluation results
│   │   ├── model-name-1/
│   │   │   ├── processing_YYYYMMDD_HHMMSS/  # Per-run results
│   │   │   │   ├── d0_wellformedness_report.xlsx
│   │   │   │   ├── d1_source_fidelity_report.xlsx
│   │   │   │   ├── d2_*_validation_report.xlsx  # Mode-dependent
│   │   │   │   ├── d3_structural_fidelity_report.xlsx
│   │   │   │   ├── d4_semantic_validation_report.xlsx
│   │   │   │   ├── unified_evaluation_summary.xlsx
│   │   │   │   ├── unified_evaluation_heatmap.png
│   │   │   │   ├── processing_metadata.json
│   │   │   │   └── json/      # JSON reports
│   │   │   │       ├── d0_wellformedness_report.json
│   │   │   │       ├── d1_source_fidelity_report.json
│   │   │   │       ├── d2_*_validation_report.json
│   │   │   │       ├── d3_structural_fidelity_report.json
│   │   │   │       ├── d4_semantic_validation_report.json
│   │   │   │       └── unified_evaluation_summary.json
│   │   │   └── processing_comparison/  # Multi-run comparison
│   │   │       └── processing_comparison.png
│   │   └── model-name-2/
│   └── cross_model/            # Cross-model comparison
│       └── cross_model_comp_*.png  # Model comparison charts
│
├── logs/                       # Evaluation logs
│   ├── d0_evaluation.log
│   ├── d1_evaluation.log
│   ├── d2_evaluation.log
│   ├── d3_evaluation.log
│   ├── d4_evaluation.log
│   ├── unified_evaluation.log
│   └── comparison.log
│
├── d0_eval.py                  # D0 evaluation script
├── d1_eval.py                  # D1 evaluation script
├── d2_eval.py                  # D2 evaluation script
├── d3_eval.py                  # D3 evaluation script
├── d4_eval.py                  # D4 evaluation script
├── unified_eval.py             # Run all dimensions + unified report
├── compare.py                  # Comparison and visualization tool
├── config.py                   # Path configuration
├── requirements.txt            # Python dependencies
└── README.md                   # This file
```

### Key Directories

- **`tei_evaluator/`**: Core evaluation framework with modular architecture
- **`data/`**: Input files, reference files, and schemas (user must provide)
  - **`data/examples/`**: Example data for testing and demonstration (included in repository) mirroring the data folder structure
- **`results/`**: Generated evaluation reports and visualizations
  - **`results/model/{model}/{processing_run}/`**: Per-run evaluation results with all dimension reports
  - **`results/model/{model}/processing_comparison/`**: Multi-run comparison visualizations
  - **`results/cross_model/`**: Cross-model comparison visualizations
- **`logs/`**: Evaluation execution logs

## Configuration

All paths and directories are managed centrally in `config.py` using the `PathConfig` class. This ensures consistent access to input, output, reference, and schema files throughout the framework.

- **Default Paths**:
  - Source text files: `data/input/` (plain text `.txt` files)
  - LLM-generated XML: `data/output/{model}/processing_YYYYMMDD_HHMMSS/` (organized by model and processing run)
  - Reference XML files: `data/references/` (gold standard files)
  - Project schemas: `data/schemas/`
  - Bundled TEI schema: `tei_evaluator/resources/schemas/tei_all.rng`
  - Jing validator: `tei_evaluator/resources/tools/jing-RELEASE220.jar`
  - Evaluation results: `results/model/{model}/{processing_run}/`
  - Logs: `logs/`

- **Example Data**:
  - The repository includes example data in `data/examples/` to help users understand the expected data structure
  - Example files include sample input texts, LLM outputs from various models (Claude, GPT, Qwen, OLMO), reference XML files, and example schemas
  - You can use these examples to test the framework or as templates for organizing your own data

- **Processing Run Structure**:
  - The framework supports timestamped processing runs: `processing_YYYYMMDD_HHMMSS/`
  - Each run can have its own XML files, metadata, and prompts
  - Legacy structure (direct XML files in model directory) is also supported
  - Evaluation results mirror the processing run structure

- **Automatic Detection**:
  - The framework auto-detects available models and processing runs in `data/output/`
  - Reference files are matched by filename in `data/references/`
  - Project schema is auto-detected as any `.rng` file in `data/schemas/`

- **Custom Configuration**:
  - You can override any path by passing explicit arguments to the evaluation scripts (see Usage section)
  - For advanced setups, subclass or modify `PathConfig` in `config.py`


## Logging

All evaluation scripts support comprehensive logging with configurable verbosity:

- **Log Location**: Logs are automatically saved to the directory configured in `config.py` (default: `logs/` at project root)
  - `logs/d0_evaluation.log` - Dimension 0 logs
  - `logs/d1_evaluation.log` - Dimension 1 logs
  - `logs/d2_evaluation.log` - Dimension 2 logs
  - `logs/d3_evaluation.log` - Dimension 3 logs
  - `logs/d4_evaluation.log` - Dimension 4 logs
  - To customize: modify `PathConfig.logs_dir` in `config.py`
- **Verbosity Control**:
  - `-v, --verbose`: Enable DEBUG level logging (detailed processing information)
  - `-q, --quiet`: Suppress console output except errors
  - `--log-level`: Set custom log level (DEBUG, INFO, WARNING, ERROR)

**Examples**:
```bash
# Standard evaluation (INFO level)
python d0_eval.py --xml-dir data/output/claude-sonnet-4-5 --output-dir results/d0

# Verbose mode for debugging
python d0_eval.py --xml-dir data/output/claude-sonnet-4-5 --output-dir results/d0 --verbose

# Quiet mode for batch processing
python d0_eval.py --xml-dir data/output/claude-sonnet-4-5 --output-dir results/d0 --quiet
```

For complete options, run: `python d0_eval.py --help` (or any other eval script)

## Usage

### Running Evaluations

Each dimension can be run independently using the provided scripts. All scripts support both **interactive** and **command-line** modes. Results are saved in the `results/` directory by default.

#### Unified Evaluation (All Dimensions)

**Interactive Mode** (default):
```bash
python unified_eval.py
```

**Command-Line Mode**:
```bash
# Run complete pipeline for all models
python unified_eval.py --mode all-models --schema-mode both

# Run complete pipeline for specific model and run
python unified_eval.py --mode complete --model gpt-5-mini --run processing_20251103_122008

# Run evaluations only (no unified report)
python unified_eval.py --mode eval-only --model gpt-5-mini

# Generate report from existing results
python unified_eval.py --mode report-only --model gpt-5-mini --run all

# With logging control
python unified_eval.py --mode all-models --quiet --log-level INFO
```

**Unified Evaluation Options**

When running the unified evaluation, you can select:
- **All models and all processing runs**: Evaluate every model and every run in your data directory
- **All processing runs from one model**: Evaluate all runs for a selected model
- **Single processing run from one model**: Evaluate a specific run for a selected model

These options are available interactively when you start the script, or via command-line arguments:
- `--mode`: Execution mode (all-models, single-model-all-runs, complete, eval-only, report-only)
- `--model`: Model name to evaluate
- `--run`: Processing run identifier
- `--schema-mode`: Schema validation mode (none, tei, project, both)
- `--interactive`: Force interactive mode
- `--verbose`: Enable DEBUG logging
- `--quiet`: Suppress console output
- `--log-level`: Set logging level (DEBUG, INFO, WARNING, ERROR)

You can also control how schema validation affects the overall score using the `schema_mode` option:
- `none`: No schema validation impact
- `tei`: Only TEI schema validation required
- `project`: Only project schema validation required
- `both`: Both TEI and project schema validation required (default)


#### Dimension 0: XML Well-formedness

**Interactive Mode**:
```bash
python d0_eval.py
```

**Command-Line Mode**:
```bash
# Specify directories
python d0_eval.py --xml-dir data/output/claude-sonnet-4-5 --output-dir results/d0

# With logging control
python d0_eval.py --xml-dir data/output/claude-sonnet-4-5  --output-dir results/d0 --verbose

# Quiet mode for batch processing
python d0_eval.py --xml-dir data/output/claude-sonnet-4-5  --output-dir results/d0 --quiet
```
- Validates XML structure and syntax
- Fast evaluation (~1ms per well-formed file)
- Options: `--xml-dir`, `--output-dir`, `--pattern`, `--verbose`, `--quiet`, `--log-level`

#### Dimension 1: Source Fidelity

**Interactive Mode**:
```bash
python d1_eval.py
```

**Command-Line Mode**:
```bash
# Specify directories
python d1_eval.py --xml-dir data/output/qwen3-14B-Q6 --txt-dir data/input --output-dir results/d1

# With logging control
python d1_eval.py --xml-dir data/output/qwen3-14B-Q6 --txt-dir data/input --output-dir results/d1 --verbose
```
- Compares XML output against original source text
- Uses plaintext input files from `data/input/`
- Options: `--xml-dir`, `--txt-dir`, `--output-dir`, `--pattern`, `--verbose`, `--quiet`, `--log-level`

#### Dimension 2: Schema Compliance

**Interactive Mode** (with validation mode selection):
```bash
python d2_eval.py
```

**Command-Line Mode**:
```bash
# Combined validation (TEI + Project schema) - default
python d2_eval.py --xml-dir data/output/olmo2-32B-instruct-Q4 --output-dir results/d2

# TEI schema validation only
python d2_eval.py --xml-dir data/output/olmo2-32B-instruct-Q4 --output-dir results/d2 --mode tei_only

# Project schema validation only
python d2_eval.py --xml-dir data/output/olmo2-32B-instruct-Q4 --output-dir results/d2 --mode project_only

# With logging control
python d2_eval.py --xml-dir data/output/olmo2-32B-instruct-Q4 --output-dir results/d2 --mode combined --verbose
```
- Validates against TEI and/or project schema
- Requires Java for Jing validator
- Three validation modes: `combined` (default), `tei_only`, `project_only`
- Options: `--xml-dir`, `--output-dir`, `--mode`, `--pattern`, `--verbose`, `--quiet`, `--log-level`

#### Dimension 3: Structural Comparison

**Interactive Mode**:
```bash
python d3_eval.py
```

**Command-Line Mode**:
```bash
# Specify directories
python d3_eval.py --xml-dir data/output/gpt-5-mini --ref-dir data/references --output-dir results/d3

# With logging control
python d3_eval.py --xml-dir data/output/gpt-5-mini --ref-dir data/references --output-dir results/d3 --verbose

# Quiet mode for batch processing
python d3_eval.py --xml-dir data/output/gpt-5-mini --ref-dir data/references --output-dir results/d3 --quiet
```
- Compares LLM output to reference files in `data/references/`
- Evaluates structural completeness and organization
- Options: `--xml-dir`, `--ref-dir`, `--output-dir`, `--pattern`, `--verbose`, `--quiet`, `--log-level`, `--interactive`

#### Dimension 4: Semantic Content Matching

**Interactive Mode**:
```bash
python d4_eval.py
```

**Command-Line Mode**:
```bash
# Auto-discovery mode (default, genre-agnostic)
python d4_eval.py --xml-dir data/output/qwen3-14B-Q6 --ref-dir data/references --output-dir results/d4

# Correspondence mode (predefined letter-specific elements)
python d4_eval.py --xml-dir data/output/qwen3-14B-Q6 --ref-dir data/references --output-dir results/d4 --mode correspondence

# With logging control
python d4_eval.py --xml-dir data/output/qwen3-14B-Q6 --ref-dir data/references --output-dir results/d4 --verbose

# Quiet mode for batch processing
python d4_eval.py --xml-dir data/output/qwen3-14B-Q6 --ref-dir data/references --output-dir results/d4 --quiet
```
- Compares element-level content to references from reference files in `data/references/`
- Supports auto-discovery (default) or correspondence mode (predefined elements)
- Options: `--xml-dir`, `--ref-dir`, `--output-dir`, `--mode`, `--pattern`, `--verbose`, `--quiet`, `--log-level`, `--interactive`

**Common Options for All Dimension Scripts**:
- `--help`: Show detailed help message with all options
- `--version`: Show script version
- `--interactive`: Force interactive mode
- `--verbose` or `-v`: Enable DEBUG level logging
- `--quiet` or `-q`: Suppress console output except errors
- `--log-level`: Set custom log level (DEBUG, INFO, WARNING, ERROR)


### Customizing Inputs/Outputs

You can specify input, reference, and output directories via command-line arguments. Examples:

```bash
# Dimension 1: Source fidelity with custom paths
python d1_eval.py --xml-dir data/output/gpt-5 --txt-dir data/input --output-dir results/custom

# Dimension 3: Structural comparison with custom paths
python d3_eval.py --xml-dir data/output/gpt-5 --ref-dir data/references --output-dir results/custom

# Using custom file patterns
python d0_eval.py --xml-dir data/output/gpt-5 --output-dir results/custom --pattern "letter*.xml"
```

### Generating Reports

Reports are automatically generated in JSON and Excel formats for each dimension. Results are organized by model and processing run:

**Per-Run Reports** (in `results/model/{model}/{processing_run}/`):
- Excel reports: `d0_wellformedness_report.xlsx`, `d1_source_fidelity_report.xlsx`, etc.
- JSON reports: `json/d0_wellformedness_report.json`, `json/d1_source_fidelity_report.json`, etc.
- Unified summary: `unified_evaluation_summary.xlsx` and `json/unified_evaluation_summary.json`
- Heatmap visualization: `unified_evaluation_heatmap.png`

**Comparison Visualizations**:
- Processing comparison (per model): `results/model/{model}/processing_comparison/processing_comparison.png`
- Cross-model comparison: `results/cross_model/cross_model_comp_{models}.png`

### Comparing Results

Use the `compare.py` script to generate comparison visualizations:

**Interactive Mode** (default):
```bash
python compare.py
```

**Command-Line Mode**:
```bash
# Internal run comparison for a single model
python compare.py --mode internal --model olmo2-32B-instruct-Q4

# Cross-model comparison
python compare.py --mode cross --models gpt-5-mini, qwen3-14B-Q6

# With logging control
python compare.py --mode internal --model claude-sonnet-4-5 --log-level DEBUG
```

**Comparison Types**:
1. **Internal Run Comparison**: Compare multiple processing runs for a single model
   - Useful for evaluating different prompts, temperatures, or configurations
   - Output: `results/model/{model}/processing_comparison/processing_comparison.png`

2. **Cross-Model Comparison**: Compare performance across different LLM models
   - Compares one representative run per model
   - Output: `results/cross_model/cross_model_comp_{models}.png`

**Options**:
- `--mode`: Comparison mode (internal, cross)
- `--model`: Model name for internal comparison
- `--models`: Comma-separated model names for cross-model comparison
- `--interactive`: Force interactive mode
- `--verbose`: Enable DEBUG logging
- `--log-level`: Set logging level
- `--help`: Show detailed help message
- `--version`: Show script version

## Output & Results

All evaluation scripts generate detailed output files in the `results/` directory, organized by model and processing run:

**Directory Structure**: `results/model/{model_name}/{processing_run}/`

**Per-Dimension Reports**:
- **Excel Reports** (root directory): Human-friendly summaries
  - `d0_wellformedness_report.xlsx` - XML syntax validation results
  - `d1_source_fidelity_report.xlsx` - Content preservation analysis
  - `d2_*_validation_report.xlsx` - Schema validation (mode-dependent naming)
  - `d3_structural_fidelity_report.xlsx` - Structural comparison results
  - `d4_semantic_validation_report.xlsx` - Semantic content matching results

- **JSON Reports** (`json/` subdirectory): Machine-readable detailed results
  - `d0_wellformedness_report.json`
  - `d1_source_fidelity_report.json`
  - `d2_*_validation_report.json`
  - `d3_structural_fidelity_report.json`
  - `d4_semantic_validation_report.json`

**Unified Reports** (root directory):
- `unified_evaluation_summary.xlsx` - Combined cross-dimensional summary
- `json/unified_evaluation_summary.json` - Machine-readable unified results
- `unified_evaluation_heatmap.png` - Visual heatmap of file-level performance

**Comparison Visualizations**:
- `results/model/{model}/processing_comparison/processing_comparison.png` - Multi-run comparison
- `results/cross_model/cross_model_comp_{models}.png` - Cross-model comparison


## Features

This release (v1.0.0) includes:

**Core Functionality**:
- ✅ Multi-dimensional evaluation framework (D0-D4) for TEI XML quality assessment
- ✅ Command-line interface (CLI) support for all evaluation scripts via `argparse`
- ✅ Comprehensive logging system with configurable verbosity levels
- ✅ Quiet mode (`--quiet`) and verbose mode (`--verbose`) for all scripts
- ✅ Dual-mode operation: Interactive menus and CLI arguments
- ✅ Centralized logging configuration in `logs/` directory

**Code Quality**:
- ✅ Google-style docstrings for all public functions, classes, and methods
- ✅ Type hints for all function signatures
- ✅ Specific exception handling (no bare `except:` blocks)
- ✅ Consistent import organization (relative imports within package)
- ✅ Standardized error handling patterns
- ✅ Enhanced module-level documentation

**Evaluation Scripts**:
- `d0_eval.py`, `d1_eval.py`, `d2_eval.py`, `d3_eval.py`, `d4_eval.py` - Individual dimension evaluations
- `unified_eval.py` - Unified multi-dimensional evaluation
- `compare.py` - Comparison and visualization tool

For detailed usage, see the Usage section above.

## License

MIT

## Citation

If you use this tool in your research, please cite:

```
[Coming soon: citation information]
```

## Contact

sabrina.strutz@uni-graz.at
